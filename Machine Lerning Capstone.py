# -*- coding: utf-8 -*-
"""Capstone_BeneranFix2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TvxKyf0IYXE688hUXOMYBL_1EBqdPzZA
"""

pip install sastrawi

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.feature_extraction.text import TfidfVectorizer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.metrics.pairwise import cosine_similarity

from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

tv = TfidfVectorizer(max_features=5000)
stem = StemmerFactory().create_stemmer()
stopword = StopWordRemoverFactory().create_stop_word_remover()

"""## Load Data"""

data_tourism_rating = pd.read_csv('tourism_rating.csv')
data_tourism_with_id = pd.read_csv('tourism_with_id.csv')
data_user = pd.read_csv('user.csv')

"""3.2 Eksplorasi Fitur-fitur pada data"""

data_tourism_rating.head()

data_tourism_with_id.head()

data_tourism_with_id

data_user.head()

"""## Data rating"""

data_tourism_rating

data_tourism_with_id.info()

print(data_tourism_rating.columns)
print(data_tourism_with_id.columns)

data_recommend = pd.merge(data_tourism_rating.groupby('Place_Id')['Place_Rating'].mean(),data_tourism_with_id,on='Place_Id')
data_recommend

"""4. Eksplorasi Data"""

import folium
wisata = folium.Map(location=[-6.175392, 106.827153], zoom_start=6)

lokasi_wis = folium.map.FeatureGroup()

for lat, lng, in zip(data_tourism_with_id.Latitude, data_tourism_with_id.Longitude):
    lokasi_wis.add_child(
        folium.CircleMarker(
            [lat, lng],
            radius=5,
            color='yellow',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
))
latitudes = list(data_tourism_with_id.Latitude)
longitudes = list(data_tourism_with_id.Longitude)
labels = list(data_tourism_with_id.Category)
for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat,lng], popup=label).add_to(wisata)
wisata.add_child(lokasi_wis)

from folium import plugins
from folium.plugins import HeatMap


wisata_map = folium.Map(location=[-6.175392, 106.827153],
                    zoom_start = 13)


data_tourism_with_id['Latitude'] = data_tourism_with_id['Latitude'].astype(float)
data_tourism_with_id['Longitude'] = data_tourism_with_id['Longitude'].astype(float)
heat_df = data_tourism_with_id[['Latitude', 'Longitude']]
heat_data = [[row['Latitude'],row['Longitude']] for index, row in heat_df.iterrows()]
# Plot it on the map
HeatMap(heat_data).add_to(wisata_map)

# Display the map
wisata_map



correlation_matrix = data_recommend.corr()

# Plot correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

correlations = data_recommend.corr()

# Find the strongest positive and negative correlations
strongest_positive_corr = correlations.unstack().sort_values().drop_duplicates().tail(5)
strongest_negative_corr = correlations.unstack().sort_values().drop_duplicates().head(5)

print("Strongest Positive Correlations:")
print(strongest_positive_corr)

print("\nStrongest Negative Correlations:")
print(strongest_negative_corr)

data_recommend

"""5. Persiapan Data untuk Pemodelan"""

# Membaca dataset untuk dilakukan encoding

data_copy1 = data_recommend.copy()

#pip install -U scikit-learn

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder

# Separate the features (X) and target variable (y)
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
X_encoded = encoder.fit_transform(data_copy1[['Place_Name', 'Category', 'Region']])
feature_names = encoder.get_feature_names_out(['Place_Name', 'Category', 'Region'])
X = pd.DataFrame(X_encoded, columns=feature_names)
y = data_copy1['Place_Id']

# Train a random forest model
model = RandomForestRegressor()
model.fit(X, y)

# Get feature importance
feature_importance = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)

# Print feature importance
print(feature_importance)

def preprocessing(data):
    data = data.lower()
    data = stem.stem(data)
    data = stopword.remove(data)
    return data

data_content_based_filtering = data_recommend.copy()
data_content_based_filtering['Tags'] = data_content_based_filtering['Location'] + ' ' + data_content_based_filtering['Category'] #Feature Engineering
data_content_based_filtering.drop(['Price','Place_Rating','Location','Category','Region'],axis=1,inplace=True)
data_content_based_filtering

data_content_based_filtering.Tags = data_content_based_filtering.Tags.apply(preprocessing)
data_content_based_filtering

Vektor = tv.fit_transform(data_content_based_filtering['Tags'])
similarity = cosine_similarity(Vektor)
print(similarity[0][1:10])

def recommend_by_content_based_filtering(template):
    template_index = data_content_based_filtering[data_content_based_filtering['Place_Name']==template].index[0]
    distancess = similarity[template_index]
    template_list = sorted(list(enumerate(distancess)),key=lambda x: x[1],reverse=True)[1:10]

    recommended_name = []
    for i in template_list:
        recommended_name.append([data_content_based_filtering.iloc[i[0]].Place_Name]+[i[1]])
        # print(nama_tempats.iloc[i[0]].original_title)

    return recommended_name

recommend_by_content_based_filtering('Locavore')

"""#6. Pemodelan Machine Learning dengan Cllaboratif Filtering"""

data_collaborative_filtering = data_tourism_rating.copy()
data_collaborative_filtering

user_ids = data_collaborative_filtering['User_Id'].unique().tolist()
print('list userID: ', user_ids)
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded opposite userID: ', user_encoded_to_user)

resto_ids = data_collaborative_filtering['Place_Id'].unique().tolist()

resto_to_resto_encoded = {x: i for i, x in enumerate(resto_ids)}

resto_encoded_to_resto = {i: x for i, x in enumerate(resto_ids)}

data_collaborative_filtering['user'] = data_collaborative_filtering['User_Id'].map(user_to_user_encoded)

data_collaborative_filtering['place'] = data_collaborative_filtering['Place_Id'].map(resto_to_resto_encoded)

num_users = len(user_to_user_encoded)
print(num_users)

num_resto = len(resto_encoded_to_resto)
print(num_resto)

data_collaborative_filtering['Place_Rating'] = data_collaborative_filtering['Place_Rating'].values.astype(np.float32)

min_rating = min(data_collaborative_filtering['Place_Rating'])

max_rating = max(data_collaborative_filtering['Place_Rating'])

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_resto, min_rating, max_rating
))

data_collaborative_filtering = data_collaborative_filtering.sample(frac=1, random_state=42)
data_collaborative_filtering

x = data_collaborative_filtering[['user', 'place']].values
y = data_collaborative_filtering['Place_Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
train_indices = int(0.8 * data_collaborative_filtering.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_resto
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.resto_embedding = layers.Embedding( # layer embeddings resto
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    resto_vector = self.resto_embedding(inputs[:, 1])
    resto_bias = self.resto_bias(inputs[:, 1])

    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2)

    x = dot_user_resto + user_bias + resto_bias
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_resto, 50)

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

def recommend_by_collaborative_filtering(user_id):
    resto_df = data_tourism_with_id
    df = data_tourism_rating
    resto_visited_by_user = df[df.User_Id == user_id]
    resto_not_visited = resto_df[~resto_df['Place_Id'].isin(resto_visited_by_user.Place_Id.values)]['Place_Id']
    resto_not_visited = list(
        set(resto_not_visited)
        .intersection(set(resto_to_resto_encoded.keys()))
    )
    resto_not_visited = [[resto_to_resto_encoded.get(x)] for x in resto_not_visited]
    user_encoder = user_to_user_encoded.get(user_id)
    user_resto_array = np.hstack( ([[user_encoder]] * len(resto_not_visited), resto_not_visited))
    ratings = model.predict(user_resto_array).flatten()
    top_ratings_indices = ratings.argsort()[-10:][::-1]
    recommended_resto_ids = [
    resto_encoded_to_resto.get(resto_not_visited[x][0]) for x in top_ratings_indices]
    return recommended_resto_ids

resto_df = data_tourism_with_id
df = data_tourism_rating

user_id = df.User_Id.sample(1).iloc[0]
resto_visited_by_user = df[df.User_Id == user_id]

resto_not_visited = resto_df[~resto_df['Place_Id'].isin(resto_visited_by_user.Place_Id.values)]['Place_Id']
    resto_not_visited = list(
        set(resto_not_visited)
        .intersection(set(resto_to_resto_encoded.keys()))
    )
    resto_not_visited = [[resto_to_resto_encoded.get(x)] for x in resto_not_visited]
    user_encoder = user_to_user_encoded.get(user_id)
    user_resto_array = np.hstack( ([[user_encoder]] * len(resto_not_visited), resto_not_visited))

ratings = model.predict(user_resto_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_resto_ids = [
    resto_encoded_to_resto.get(resto_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Resto with high ratings from user')
print('----' * 8)

top_resto_user = (
    resto_visited_by_user.sort_values(
        by = 'Place_Rating',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

resto_df_rows = resto_df[resto_df['Place_Id'].isin(top_resto_user)]
for row in resto_df_rows.itertuples():
    print(row.Place_Name)

print('----' * 8)
print('Top 10 Destination recommendation')
print('----' * 8)

recommended_resto = resto_df[resto_df['Place_Id'].isin(recommended_resto_ids)]
for row in recommended_resto.itertuples():
    print(row.Place_Name)

recommend_by_content_based_filtering('Prancak Beach')

user_id = 215
resto_visited_by_user = df[df.User_Id == user_id]
top_resto_user = (
    resto_visited_by_user.sort_values(
        by = 'Place_Rating',
        ascending=False
    )
    .head(1)
    .Place_Id.values
)
user_preferences=""
resto_df_rows = resto_df[resto_df['Place_Id'].isin(top_resto_user)]
for row in resto_df_rows.itertuples():
    user_preferences=(row.Place_Name)
print(user_preferences)
# Generate collaborative filtering recommendations
collaborative_filtering_recommendations=[]
recommend_resto_ids= recommend_by_collaborative_filtering(user_id)
recommended_resto = resto_df[resto_df['Place_Id'].isin(recommended_resto_ids)]
for row in recommended_resto.itertuples():
    collaborative_filtering_recommendations.append(row.Place_Name)
# Generate content-based filtering recommendations
content_based_filtering_recommendations = recommend_by_content_based_filtering(user_preferences)

# Combine recommendations from both methods
combined_recommendations = collaborative_filtering_recommendations + content_based_filtering_recommendations

print(combined_recommendations)

def evaluate_precision_recall(recommended_items, relevant_items):
    true_positives = len(set(recommended_items) & set(relevant_items))
    false_positives = len(recommended_items) - true_positives
    false_negatives = len(relevant_items) - true_positives

    precision = true_positives / (true_positives + false_positives)
    recall = true_positives / (true_positives + false_negatives)

    return precision, recall

def calculate_average_precision(recommended_items, relevant_items):
    average_precision = 0.0
    num_relevant_items = len(relevant_items)
    true_positives = 0

    for i, item in enumerate(recommended_items):
        if item in relevant_items:
            true_positives += 1
            precision = true_positives / (i + 1)
            average_precision += precision

    if true_positives > 0:
        average_precision /= num_relevant_items

    return average_precision

print(combined_recommendations)

